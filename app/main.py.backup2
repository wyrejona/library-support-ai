from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.templating import Jinja2Templates
import os
import shutil
from datetime import datetime
from typing import List, Optional
import uvicorn
from pathlib import Path
import subprocess
import asyncio
import sys
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add the app directory to Python path
current_file = Path(__file__).resolve()
app_dir = current_file.parent
project_root = app_dir.parent

sys.path.insert(0, str(app_dir))
sys.path.insert(0, str(project_root))

logger.info(f"Current directory: {current_file}")
logger.info(f"App directory: {app_dir}")
logger.info(f"Project root: {project_root}")

# Import configuration FIRST
try:
    from app.config import config
    logger.info("✓ Configuration loaded")
except ImportError as e:
    logger.error(f"Failed to import config: {e}")
    # Create minimal config
    class MinimalConfig:
        def __init__(self):
            self.chat_model = "qwen:0.5b"
            self.embedding_model = "nomic-embed-text:latest"
            self.pdfs_dir = Path("pdfs")
            self.data_dir = Path("data")
            self.templates_dir = Path("app/templates")
            self.project_root = Path(".")
    
    config = MinimalConfig()

# Import our custom modules with fallbacks
VectorStore = None
OllamaClient = None

try:
    logger.info("Trying to import from app.utils and app.ai.llm...")
    from app.utils import VectorStore
    from app.ai.llm import OllamaClient
    logger.info("✓ Imported from app.*")
except ImportError as e:
    logger.warning(f"First import failed: {e}")
    try:
        logger.info("Trying to import from utils and ai.llm...")
        from utils import VectorStore
        from ai.llm import OllamaClient
        logger.info("✓ Imported from direct modules")
    except ImportError as e:
        logger.warning(f"Second import failed: {e}")
        try:
            logger.info("Trying relative imports...")
            from .utils import VectorStore
            from .ai.llm import OllamaClient
            logger.info("✓ Imported with relative imports")
        except ImportError as e:
            logger.error(f"All imports failed: {e}")
            logger.info("Creating dummy implementations...")
            
            # Dummy implementations
            class DummyVectorStore:
                def __init__(self, *args, **kwargs):
                    logger.warning("Using DummyVectorStore")
                def search(self, *args, **kwargs):
                    return []
                def load(self):
                    return False
            
            class DummyOllamaClient:
                def __init__(self, model="qwen:0.5b"):
                    logger.warning(f"Using DummyOllamaClient with model={model}")
                    self.model = model
                def generate_response(self, prompt, context):
                    return f"Dummy response to: {prompt}. Please check your imports and dependencies."
            
            VectorStore = DummyVectorStore
            OllamaClient = DummyOllamaClient

# Initialize components
try:
    vector_store = VectorStore()
    llm_client = OllamaClient()
    logger.info(f"✓ Components initialized with model: {config.chat_model}")
except Exception as e:
    logger.error(f"Failed to initialize components: {e}")
    # Use dummy implementations if initialization fails
    vector_store = VectorStore()
    llm_client = OllamaClient()

# Initialize FastAPI with config values
app = FastAPI(
    title=config.app_name,
    version=config.app_version,
    description="University of Embu Library Support AI"
)

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Get directories from config
pdfs_dir = config.pdfs_dir
data_dir = config.data_dir
templates_dir = config.templates_dir

logger.info(f"PDFs directory: {pdfs_dir}")
logger.info(f"Data directory: {data_dir}")
logger.info(f"Templates directory: {templates_dir}")

# Configure templates
if not templates_dir.exists():
    logger.warning(f"Templates directory does not exist! Creating...")
    os.makedirs(templates_dir, exist_ok=True)

templates = Jinja2Templates(directory=str(templates_dir))

def format_file_size(bytes):
    """Format file size in human readable format"""
    if bytes == 0:
        return "0 Bytes"
    size_names = ["Bytes", "KB", "MB", "GB", "TB"]
    i = 0
    while bytes >= 1024 and i < len(size_names) - 1:
        bytes /= 1024.0
        i += 1
    return f"{bytes:.2f} {size_names[i]}"

# ==================== ROUTES ====================

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """Home page"""
    logger.info("GET / called")
    return templates.TemplateResponse("index.html", {
        "request": request,
        "app_name": config.app_name,
        "app_version": config.app_version,
        "current_model": config.chat_model
    })

# File Management Page - GET request returns HTML
@app.get("/files", response_class=HTMLResponse)
async def manage_files(request: Request):
    """File management page"""
    logger.info("GET /files called")
    
    # Get list of files
    files = []
    for f in os.listdir(pdfs_dir):
        if f.endswith(".pdf"):
            file_path = pdfs_dir / f
            files.append({
                "name": f,
                "size": os.path.getsize(file_path),
                "modified": datetime.fromtimestamp(os.path.getmtime(file_path)),
                "formatted_size": format_file_size(os.path.getsize(file_path))
            })
    
    # Sort files by modification time (newest first)
    files.sort(key=lambda x: x["modified"], reverse=True)
    
    # Get vector store status
    vector_status = "Not processed"
    if os.path.exists(config.vector_store_path / "index.bin"):
        vector_status = "Ready"
    
    total_size = sum(f["size"] for f in files)
    
    logger.info(f"Returning {len(files)} files to template")
    return templates.TemplateResponse("files.html", {
        "request": request,
        "files": files,
        "total_files": len(files),
        "total_size": total_size,
        "format_file_size": format_file_size,
        "vector_status": vector_status,
        "current_model": config.chat_model,
        "embedding_model": config.embedding_model
    })

# Chat Page - GET request returns HTML
@app.get("/chat", response_class=HTMLResponse)
async def chat_page(request: Request):
    """Chat page"""
    logger.info("GET /chat called")
    
    # Get list of files
    files = []
    for f in os.listdir(pdfs_dir):
        if f.endswith(".pdf"):
            file_path = pdfs_dir / f
            files.append({
                "name": f,
                "size": os.path.getsize(file_path),
                "formatted_size": format_file_size(os.path.getsize(file_path))
            })
    
    logger.info(f"Returning chat with {len(files)} files")
    return templates.TemplateResponse("chat.html", {
        "request": request,
        "files": files,
        "total_files": len(files),
        "current_model": config.chat_model
    })

# Chat API - POST request returns JSON
@app.post("/chat")
async def chat_api(request_data: dict):
    """Chat API endpoint"""
    user_message = request_data.get("message") or request_data.get("query") or ""
    logger.info(f"POST /chat called with message: {user_message[:50]}...")
    
    if not user_message:
        return {"response": "Please enter a question."}
    
    try:
        # Search for relevant context
        search_results = vector_store.search(user_message, k=config.search_default_k)
        logger.info(f"Search returned {len(search_results)} results")
        
        try:
            from app.utils import format_context
            context = format_context(search_results, max_length=config.max_context_length)
        except ImportError:
            context = "\n".join([str(r) for r in search_results])
        
        # Generate response using LLM
        response = llm_client.generate_response(prompt=user_message, context=context)
        logger.info(f"Generated response length: {len(response)}")
        
        return {
            "response": response,
            "answer": response,
            "context_used": len(context) > 0,
            "model_used": config.chat_model
        }
    except Exception as e:
        logger.error(f"Chat error: {e}", exc_info=True)
        return {
            "response": f"I apologize, but I encountered an error: {str(e)}",
            "error": str(e),
            "model_used": config.chat_model
        }

@app.post("/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    """Upload PDF files"""
    uploaded_files = []
    
    for file in files:
        if file.filename.lower().endswith('.pdf'):
            file_path = pdfs_dir / file.filename
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            file_size = os.path.getsize(file_path)
            uploaded_files.append({
                "name": file.filename,
                "size": file_size,
                "uploaded_at": datetime.now().isoformat()
            })
    
    logger.info(f"Uploaded {len(uploaded_files)} files")
    return {"status": "success", "uploaded": uploaded_files}

@app.get("/api/files")
async def list_files_api():
    """API endpoint to list files"""
    files = []
    for f in os.listdir(pdfs_dir):
        if f.endswith(".pdf"):
            file_path = pdfs_dir / f
            files.append({
                "name": f,
                "size": os.path.getsize(file_path),
                "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
            })
    return {"files": files}

@app.get("/download/{filename}")
async def download_file(filename: str):
    """Download a file"""
    filepath = pdfs_dir / filename
    if os.path.exists(filepath):
        logger.info(f"Downloading file: {filename}")
        return FileResponse(
            path=filepath,
            filename=filename,
            media_type='application/pdf',
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
    logger.warning(f"File not found: {filename}")
    raise HTTPException(status_code=404, detail="File not found")

@app.delete("/files/{filename}")
async def delete_file(filename: str):
    """Delete a file"""
    path = pdfs_dir / filename
    if os.path.exists(path):
        os.remove(path)
        logger.info(f"Deleted file: {filename}")
        return {"status": "deleted", "filename": filename}
    logger.warning(f"File not found for deletion: {filename}")
    raise HTTPException(status_code=404, detail="File not found")

@app.delete("/clear-all-files")
async def clear_all_files():
    """Clear all files"""
    try:
        # Remove all PDF files
        for filename in os.listdir(pdfs_dir):
            if filename.endswith(".pdf"):
                os.remove(pdfs_dir / filename)
        
        # Clear vector store data
        if os.path.exists(config.vector_store_path):
            shutil.rmtree(config.vector_store_path)
            os.makedirs(config.vector_store_path, exist_ok=True)
        
        logger.info("Cleared all files")
        return {"status": "success", "message": "All files cleared"}
    except Exception as e:
        logger.error(f"Error clearing files: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ingest")
async def ingest_pdfs():
    """Process PDFs for vector store"""
    import subprocess
    try:
        # Run ingest.py from the project root
        ingest_script = project_root / "ingest.py"
        logger.info(f"Running ingest script: {ingest_script}")
        result = subprocess.run(["python3", str(ingest_script)], capture_output=True, text=True)
        if result.returncode == 0:
            logger.info("PDFs processed successfully")
            return {"success": True, "message": "PDFs processed successfully"}
        else:
            logger.error(f"Ingest failed: {result.stderr}")
            return {"success": False, "error": result.stderr}
    except Exception as e:
        logger.error(f"Error in ingest: {e}")
        return {"success": False, "error": str(e)}

# Configuration endpoints
@app.get("/config")
async def get_configuration():
    """Get current configuration"""
    available_models = config.get_available_models()
    return {
        "current": {
            "chat_model": config.chat_model,
            "embedding_model": config.embedding_model,
            "ollama_base_url": config.ollama_base_url,
            "ollama_timeout": config.ollama_timeout,
            "ollama_temperature": config.ollama_temperature
        },
        "paths": {
            "pdfs_dir": str(config.pdfs_dir),
            "data_dir": str(config.data_dir),
            "templates_dir": str(config.templates_dir),
            "vector_store_path": str(config.vector_store_path)
        },
        "available_models": available_models,
        "config_file": str(config._config_file.absolute())
    }

@app.put("/config/model")
async def update_model(model_config: dict):
    """Update model configuration"""
    try:
        chat_model = model_config.get("chat_model")
        embedding_model = model_config.get("embedding_model")
        base_url = model_config.get("base_url")
        
        updates = {}
        success_messages = []
        
        # Update chat model if provided
        if chat_model:
            if config.validate_model(chat_model):
                if config.update_config("ollama", "chat_model", chat_model):
                    global llm_client
                    llm_client = OllamaClient(model=chat_model)
                    updates["chat_model"] = chat_model
                    success_messages.append(f"Chat model updated to {chat_model}")
                else:
                    return {"error": f"Failed to update chat model"}
            else:
                available = config.get_available_models()
                return {
                    "error": f"Model '{chat_model}' not found",
                    "available_chat_models": available["chat_models"]
                }
        
        # Update embedding model if provided
        if embedding_model:
            if config.validate_model(embedding_model):
                if config.update_config("ollama", "embedding_model", embedding_model):
                    global vector_store
                    vector_store = VectorStore(embedding_model=embedding_model)
                    updates["embedding_model"] = embedding_model
                    success_messages.append(f"Embedding model updated to {embedding_model}")
                else:
                    return {"error": f"Failed to update embedding model"}
            else:
                available = config.get_available_models()
                return {
                    "error": f"Model '{embedding_model}' not found",
                    "available_embedding_models": available["embedding_models"]
                }
        
        # Update base URL if provided
        if base_url:
            if config.update_config("ollama", "base_url", base_url):
                updates["base_url"] = base_url
                success_messages.append(f"Ollama base URL updated to {base_url}")
        
        if updates:
            return {
                "success": True,
                "message": " | ".join(success_messages),
                "updates": updates
            }
        else:
            return {"error": "No valid updates provided"}
            
    except Exception as e:
        logger.error(f"Failed to update model config: {e}")
        return {"error": str(e)}

@app.get("/config/reload")
async def reload_config():
    """Reload configuration from file"""
    try:
        config.reload()
        # Reinitialize components with new config
        global vector_store, llm_client
        vector_store = VectorStore()
        llm_client = OllamaClient()
        
        return {"success": True, "message": "Configuration reloaded"}
    except Exception as e:
        logger.error(f"Failed to reload config: {e}")
        return {"error": str(e)}

@app.post("/install-model")
async def install_model(model_data: dict):
    """Install an Ollama model"""
    model_name = model_data.get("model")
    pull_command = model_data.get("pull_command", f"ollama pull {model_name}")
    
    if not model_name:
        return {"error": "Model name is required"}
    
    try:
        logger.info(f"Installing model: {model_name}")
        
        # Run ollama pull command
        # Using subprocess with timeout to avoid hanging
        process = await asyncio.create_subprocess_shell(
            pull_command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=600)  # 10 minute timeout
        
        if process.returncode == 0:
            logger.info(f"Successfully installed model: {model_name}")
            return {
                "success": True,
                "message": f"Model {model_name} installed successfully",
                "stdout": stdout.decode(),
                "stderr": stderr.decode()
            }
        else:
            error_msg = stderr.decode() if stderr else "Unknown error"
            logger.error(f"Failed to install model {model_name}: {error_msg}")
            return {
                "success": False,
                "error": f"Failed to install model: {error_msg}"
            }
            
    except asyncio.TimeoutError:
        logger.error(f"Timeout installing model: {model_name}")
        return {
            "success": False,
            "error": "Installation timed out after 10 minutes"
        }
    except Exception as e:
        logger.error(f"Error installing model {model_name}: {e}")
        return {
            "success": False,
            "error": f"Installation error: {str(e)}"
        }

# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    # Check Ollama connection
    ollama_healthy = False
    try:
        import requests
        response = requests.get(f"{config.ollama_base_url}/api/tags", timeout=5)
        ollama_healthy = response.status_code == 200
    except:
        pass
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "pdfs_count": len([f for f in os.listdir(pdfs_dir) if f.endswith(".pdf")]),
        "vector_store_ready": os.path.exists(config.vector_store_path / "index.bin"),
        "ollama_connected": ollama_healthy,
        "current_model": config.chat_model,
        "embedding_model": config.embedding_model
    }

# ==================== MAIN ====================

if __name__ == "__main__":
    logger.info(f"Starting {config.app_name} v{config.app_version}")
    logger.info(f"Server: {config.server_host}:{config.server_port}")
    logger.info(f"Chat model: {config.chat_model}")
    logger.info(f"Embedding model: {config.embedding_model}")
    logger.info(f"Ollama URL: {config.ollama_base_url}")
    
    uvicorn.run(
        app,
        host=config.server_host,
        port=config.server_port,
        log_level="info",
        reload=config.debug
    )
