#!/usr/bin/env python3
"""
OPTIMIZED Vector Store with batch processing for faster embeddings
"""
import os
import pickle
import numpy as np
from typing import List, Dict, Any, Optional
import faiss
import requests
import json
from datetime import datetime
import re
import concurrent.futures
import time

class VectorStore:
    def __init__(self, embedding_model: str = "nomic-embed-text:latest"):
        self.embedding_url = "http://localhost:11434/api/embeddings"
        self.embedding_model = embedding_model
        self.index = None
        self.metadata = []
    
    def create_index(self, texts: List[str], metadata: List[Dict[str, Any]]):
        """Create index using Ollama embeddings with BATCH processing"""
        print(f"üîß Generating embeddings for {len(texts)} chunks using {self.embedding_model}...")
        
        # Process in smaller batches to avoid timeout
        batch_size = 10  # Smaller batches are more reliable
        embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(texts))
            batch_texts = texts[start_idx:end_idx]
            batch_metadata = metadata[start_idx:end_idx]
            
            print(f"  Processing batch {batch_num + 1}/{total_batches} ({len(batch_texts)} chunks)...")
            
            batch_embeddings = self._process_batch(batch_texts, batch_metadata)
            embeddings.extend(batch_embeddings)
            
            # Small delay between batches to not overload Ollama
            if batch_num < total_batches - 1:
                time.sleep(1)
        
        # Filter out failed embeddings
        valid_embeddings = []
        valid_metadata = []
        
        for emb, meta in zip(embeddings, metadata):
            if emb is not None and np.any(emb != 0):
                valid_embeddings.append(emb)
                valid_metadata.append(meta)
        
        if not valid_embeddings:
            print("‚ùå No valid embeddings generated!")
            return
        
        # Convert to numpy array
        embeddings_array = np.array(valid_embeddings).astype('float32')
        
        # Normalize for cosine similarity
        faiss.normalize_L2(embeddings_array)
        
        # Create FAISS index
        dimension = embeddings_array.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(embeddings_array)
        
        self.metadata = valid_metadata
        self._save()
        
        print(f"‚úÖ Index created: {len(valid_metadata)} documents, {dimension} dimensions")
        print(f"   Success rate: {len(valid_metadata)}/{len(texts)} ({len(valid_metadata)/len(texts)*100:.1f}%)")
    
    def _process_batch(self, batch_texts: List[str], batch_metadata: List[Dict[str, Any]]) -> List[np.ndarray]:
        """Process a batch of texts with error handling"""
        batch_embeddings = []
        
        for i, (text, meta) in enumerate(zip(batch_texts, batch_metadata)):
            try:
                # Add metadata to text for better embeddings
                enhanced_text = self._enhance_text(text, meta)
                
                response = requests.post(
                    self.embedding_url,
                    json={"model": self.embedding_model, "prompt": enhanced_text},
                    timeout=30  # Increased timeout
                )
                
                if response.status_code == 200:
                    embedding = response.json()["embedding"]
                    batch_embeddings.append(embedding)
                else:
                    print(f"    ‚ö†Ô∏è  Failed for chunk {i}: HTTP {response.status_code}")
                    # Use simple fallback: zeros
                    batch_embeddings.append(np.zeros(768))
                    
            except requests.exceptions.Timeout:
                print(f"    ‚ö†Ô∏è  Timeout for chunk {i}, using fallback")
                batch_embeddings.append(np.zeros(768))
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Error for chunk {i}: {str(e)[:50]}...")
                batch_embeddings.append(np.zeros(768))
        
        return batch_embeddings
    
    def _enhance_text(self, text: str, metadata: Dict[str, Any]) -> str:
        """Add metadata to text for better embeddings"""
        enhanced = []
        
        # Add source info
        source = metadata.get('source', '')
        if source:
            enhanced.append(f"[From: {source}]")
        
        # Add content type hints
        if metadata.get('is_procedure', False):
            enhanced.append("[Procedure]")
        if metadata.get('is_critical', False):
            enhanced.append("[Important]")
        
        # Add the text itself
        enhanced.append(text)
        
        return " ".join(enhanced)
    
    def _save(self):
        """Save to disk"""
        os.makedirs("vector_store", exist_ok=True)
        faiss.write_index(self.index, "vector_store/index.bin")
        
        save_data = {
            'metadata': self.metadata,
            'config': {
                'embedding_model': self.embedding_model,
                'created_at': datetime.now().isoformat(),
                'doc_count': len(self.metadata)
            }
        }
        
        with open("vector_store/metadata.pkl", 'wb') as f:
            pickle.dump(save_data, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        print(f"üíæ Saved vector store to: vector_store/")
    
    def load(self) -> bool:
        """Load from disk"""
        try:
            if not os.path.exists("vector_store/index.bin"):
                print("‚ùå Vector store not found. Run 'python ingest.py' first.")
                return False
            
            self.index = faiss.read_index("vector_store/index.bin")
            
            with open("vector_store/metadata.pkl", 'rb') as f:
                data = pickle.load(f)
                self.metadata = data['metadata']
            
            print(f"‚úÖ Loaded vector store: {len(self.metadata)} documents")
            return True
            
        except Exception as e:
            print(f"‚ùå Load error: {e}")
            return False
    
    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Search with enhanced query"""
        if not self.load():
            return []
        
        # Enhance query
        enhanced_query = self._enhance_query(query)
        
        try:
            response = requests.post(
                self.embedding_url,
                json={"model": self.embedding_model, "prompt": enhanced_query},
                timeout=15
            )
            
            if response.status_code != 200:
                print(f"‚ùå Query embedding failed: {response.status_code}")
                return []
            
            query_embedding = np.array([response.json()["embedding"]]).astype('float32')
            faiss.normalize_L2(query_embedding)
            
            # Search
            scores, indices = self.index.search(query_embedding, k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx != -1 and idx < len(self.metadata):
                    result = self.metadata[idx].copy()
                    result['similarity_score'] = float(score)
                    results.append(result)
            
            return results
            
        except Exception as e:
            print(f"‚ùå Search error: {e}")
            return []
    
    def _enhance_query(self, query: str) -> str:
        """Enhance query for better embeddings"""
        query_lower = query.lower()
        
        # Add context based on query type
        if any(word in query_lower for word in ['how', 'procedure', 'step', 'guide']):
            return f"Instructions for: {query}"
        elif any(word in query_lower for word in ['what', 'explain', 'define']):
            return f"Explanation of: {query}"
        elif any(word in query_lower for word in ['when', 'hour', 'time', 'schedule']):
            return f"Schedule information: {query}"
        else:
            return query
    
    def quick_test(self):
        """Quick test of the vector store"""
        if not self.load():
            return False
        
        test_queries = [
            "library hours",
            "borrowing rules",
            "plagiarism",
            "e-resources"
        ]
        
        print("\nüîç Quick test searches:")
        for query in test_queries:
            results = self.search(query, k=1)
            if results:
                print(f"  '{query}': ‚úì Found {len(results)} result(s)")
            else:
                print(f"  '{query}': ‚úó No results")
        
        return True


def format_context(search_results: List[Dict[str, Any]], max_length: int = 3000) -> str:
    """Format search results for LLM"""
    if not search_results:
        return "No relevant information found in the library documents."
    
    context_parts = []
    total_length = 0
    
    for i, result in enumerate(search_results, 1):
        # Build source info
        source_info = []
        if result.get('source'):
            source_info.append(result['source'])
        if result.get('page'):
            source_info.append(f"page {result['page']}")
        
        source_str = " | ".join(source_info)
        score = result.get('similarity_score', 0)
        
        # Format the chunk
        chunk = f"[Result {i}"
        if source_str:
            chunk += f" | Source: {source_str}"
        chunk += f" | Relevance: {score:.3f}]\n"
        chunk += f"{result['content']}\n"
        chunk += "-" * 50 + "\n"
        
        # Check length limit
        if total_length + len(chunk) > max_length:
            remaining = max_length - total_length
            if remaining > 100:
                # Truncate this chunk
                chunk = f"[Result {i} | Source: {source_str} | Relevance: {score:.3f}]\n"
                chunk += f"{result['content'][:remaining-100]}...\n"
                chunk += "-" * 50 + "\n"
                context_parts.append(chunk)
                context_parts.append(f"\n[... {len(search_results)-i} more results truncated]")
            break
        
        context_parts.append(chunk)
        total_length += len(chunk)
    
    return "".join(context_parts)


# Simple helper functions
def query_intent_analyzer(query: str) -> Dict[str, Any]:
    return {'k': 5}

def smart_search(vector_store: VectorStore, query: str) -> List[Dict[str, Any]]:
    return vector_store.search(query, k=5)
